#+TITLE: Deep pixelwise features for semantic segmentation of ultrahigh carbon steel microstructures
#+AUTHOR: 

#+OPTIONS:   H:4 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:nil pri:nil tags:not-in-toc

# use figure* environments for figures that should span both columns
# #+LaTeX_CLASS_OPTIONS: [twocolumn]

#+LATEX_HEADER: \usepackage{microtype}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \graphicspath{{figures/}}

#+BEGIN_ABSTRACT
We apply a deep convolutional neural network segmentation model to enable novel automated microstructure segmentation applications for complex microstructures typically evaluated manually and subjectively.
We explore two microstructure segmentation tasks in an openly-available ultrahigh carbon steel microstructure dataset\cite{decost2017uhcshb}: segmenting cementite particles in the spheroidized matrix, and segmenting larger fields of view featuring grain boundary carbide, spheroidized particle matrix, particle-free grain boundary denuded zone, Widmanstätten cementite.
We also demonstrate how these data-driven microstructure segmentation models can be combined to obtain empirical cementite particle size distributions from more complex micrographs containing multiple microconstituents.
#+END_ABSTRACT

* Introduction
Openly available microstructure dataset \cite{decost2017uhcshb,hecht2017}.

Our primary contributions are:
- Establishing two novel microstructure segmentation benchmark datasets
- Connecting microstructure science to the deep semantic segmentation literature
- Exploring novel means of expanding contemporary quantitative microstructure measurement techniques to more complex structures

* Methods
** Segmentation model
*** PixelNet architecture
We use a PixelNet \cite{bansal2017} architecture.
General description of training with sparse upsampling (link to our to-be-published implementation in a footnote).
General description of inference with dense upsampling.
Brief comparison with e.g. SegNet.
Motivation: training from scratch on a small dataset.

- BatchNorm - Conv - ReLU layers
- Specify layer configurations, including skip connections

TODO: update architecture schematic with learned-from-scratch models, and s/VGG/conv/

\begin{figure}[!htbp]
  \frame{
  \includegraphics[width=\textwidth]{architecture}}
  \caption{Inspiration: PixelNet. Top: semantic microstructure segmentation based on manually annotated UHCS microconstituents, including proeutectoid grain boundary cementite (light blue), ferritic matrix (dark blue), spheroidite particles (yellow), and Widmanstätten cementite (green). Bottom: Spheroidite particle segmentation with semiautomated annotations\cite{hecht2017}.}
  \label{fig:architecture}
\end{figure}

*** Training
- Initialization
- Pixel sampling strategies (random vs balanced? whatever we stick with...)
- Adam (or SGD+Nesterov with cyclic learning rate if it works?) (specify learning rate, etc).
- Dropout and regularization (weight decay)
- Specify loss function
- Label smoothing (if any in the final version)
- Document data augmentation (rotation with mirror padding/boundary conditions, flips, and scaling for now)

** Semi-automated spheroidite particle annotation
Matt's ActaMat paper \cite{hecht2017}; describe ImageJ workflow.

** Manual labeling for abstract microconstituent segmentation
We manually labeled a subset of the UHCS dataset\cite{uhcsdb,uhcsdata} using the open source medical image annotation toolkit MITK \cite{mitk}.

** Performance evaluation
We report the standard evaluation metrics for semantic segmentation tasks: pixel accuracy (AC) and region intersection over union (IU), both for individual classes and averaged over all four microstructure classes.
For each of these metrics, a higher score indicates better performance.
The intersection over union metric $IU(c)$ for class $c$ is the ratio of correctly predicted pixels of class $c$ to the union of pixels with either ground truth or predicted class $c$:

\begin{equation}
IU(c) = \frac{\sum_i (o_i == c \land y_i == c)}{\sum_i (o_i == c \lor y_i == c) }
\end{equation}

where $\land$ denotes logical conjunction (logical and) and $\lor$ denotes inclusive disjunction (logical or), $o_i$ are the predictions for each pixel $i$, and $y_i$ are the ground truth labels for each pixel.

For the spheroidite particle segmentation task, we also report performance metrics comparing particle size distributions obtained from the model predictions with those obtained from the ground truth annotations (as reported in \cite{hecht2017}.
The PSD KS metric indicates failure rates for  the two-sample Kolmogorov-Smirnov test to each pair of model and ground truth PSDs (lower is better).

** Computing denuded zone widths


* Results and Discussion
# add validation predictions for the entire dataset as supplemental figures?
** Spheroidite particle segmentation
\begin{figure}[!htbp]
  \includegraphics[width=\textwidth]{validation_predictions_spheroidite_00}
  \caption{Independent test set predictions for the spheroidite particle segmentation task.}
  \label{fig:spheroiditeresults}
\end{figure}

TODO: consider watershedding particle prediction maps before comparing PSD with annotations.

#+CAPTION: Segmentation performance on validation sets
#+NAME: tab:segmentationperf
| model                            | matrix        | spheroidite   | IU_{avg}      | ACC           | PSD \chi^2 | PSD KS |
|----------------------------------+---------------+---------------+---------------+---------------+------------+--------|
| otsu                             | 86.2 \pm 7.2  | 53.7 \pm 12.1 | 69.9 \pm 9.3  | 88.1 \pm 6.1  |            |        |
| thresholded blur\cite{hecht2017} | 0             | 0             | 0             | 0             |            |        |
|----------------------------------+---------------+---------------+---------------+---------------+------------+--------|
| conv-{1_2,2_2,3_3,4_3,5_3}       | 78.0 \pm 28.6 | 52.1 \pm 20.0 | 65.0 \pm 23.0 | 81.1 \pm 24.2 |            |        |

  
** Semantic microconstituent segmentation
# big question: how many micrographs do I need to annotate to get good perf?
# Should we try to answer this question in the current study, or down the road a bit?

\begin{figure}[!htbp]
  \includegraphics[width=\textwidth]{validation_predictions_uhcs_05}
  \caption{Validation set predictions for the complex microconstituent segmentation task.}
  \label{fig:microconstituentresults}
\end{figure}


#+CAPTION: Semantic segmentation performance averaged over LOOCV validation images. Uncertainties are sample standard deviations calculated across validation folds.
#+NAME: tab:segmentationperf
| metric             | VGG conv-{1_2,2_2,3_3,4_3,5_3} | train         |
|--------------------+--------------------------------+---------------|
| matrix             | 47.4 \pm 14.6                  | 50.5 \pm 14.2 |
| network            | 65.8 \pm 20.4                  | 69.6 \pm 18.8 |
| spheroidite        | 87.9 \pm 6.9                   | 89.1 \pm 5.9  |
| widmanstatten      | 38.3 \pm 14.2                  | 40.4 \pm 14.2 |
| IU_{avg}           | 59.8 \pm 11.4                  | 62.4 \pm 10.7 |
| ACC                | 86.4 \pm 6.9                   | 87.9 \pm 5.8  |



# | metric             | VGG conv-{1_2,2_2,3_3,4_3,5_3} |  train            |
# |--------------------+--------------------------------+-------------------|
# | IU_{matrix}        | 0.4308 \pm 0.1429              | 0.6289 \pm 0.0219 |
# | IU_{network}       | 0.6562 \pm 0.1973              | 0.8410 \pm 0.0217 |
# | IU_{spheroidite}   | 0.8604 \pm 0.0791              | 0.8968 \pm 0.0413 |
# | IU_{widmanstatten} | 0.3356 \pm 0.1375              | 0.3909 \pm 0.1182 |
# | IU_{avg}           | 0.5707 \pm 0.1086              | 0.6894 \pm 0.0311 |
# | ACC                | 0.8510 \pm 0.0733              | 0.9011 \pm 0.0313 |


# | model                          | network | denuded zone | matrix | widmanstätten | global AC |
# |--------------------------------+---------+--------------+--------+---------------+-----------|
# | VGG conv-{1_2,2_2,3_3,4_3,5_3} |       0 | 0            | 0      | 0             | 0         |






** Automated measurement of abstract microstructure
# note: change this to input, class predictions, masked particle predictions.
# use the same micrographs as in the abstract microstructure segmentation task.
\begin{figure}[!htbp]
  \includegraphics[width=\textwidth]{fused_predictions_00}
  \caption{Independent test set predictions for spheroidite segmentation results in micrographs with multiple microconstituents.}
  \label{fig:fused}
\end{figure}

TODO: add a figure showing measured particle size distributions from Figure \ref{fig:fused}.

TODO: add a figure comparing measured denuded zone widths using ground truth maps and validation set predictions as input.
Compare with Matt's manual annotations where appropriate?

* Conclusions

\section*{Acknowledgements}
We gratefully acknowledge funding for this work through National Science Foundation grants DMR-1307138 and DMR-1501830, and through the John and Claire Bertucci Foundation.
The UHCS micrographs were graciously provided by Matthew Hecht, Yoosuf Picard, and Bryan Webler (CMU)\cite{decost2017uhcshb}.
The spheroidite annotations were graciously provided by Matthew Hecht and Txai Sibley.
The open source software projects Scikit-Learn\cite{sklearn} and keras\cite{keras} were essential to this work.

\bibliographystyle{unsrt}
\bibliography{uhcs-segment}

